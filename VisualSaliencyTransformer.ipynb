{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3aab1e0-c77a-4f95-9607-b2af06574aad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c854cbc2-4bb0-4278-89fd-e0f96a875908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using Pkg; installed = Pkg.installed()\n",
    "# for p in (\"Images\", \"Colors\", \"Plots\", \"IterTools\", \"PyCall\", \"Einsum\")\n",
    "#     haskey(installed,p) || Pkg.add(p)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9dee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using PyCall\n",
    "using Knet\n",
    "using Knet.CUDA\n",
    "using Knet: atype\n",
    "using Einsum\n",
    "@pyimport torch\n",
    "@pyimport torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558ad121-f5a7-47e7-bd8b-b159829f4baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{CuDevice}:\n",
       " CuDevice(0): NVIDIA GeForce RTX 3090\n",
       " CuDevice(1): NVIDIA GeForce RTX 3090"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66829cff-7f7d-4c7d-9724-3f403116c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  4 21:09:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:17:00.0 Off |                  N/A |\n",
      "| 30%   41C    P8    19W / 350W |    264MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:B3:00.0 Off |                  N/A |\n",
      "| 47%   64C    P2   122W / 350W |  24180MiB / 24259MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1028      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      1646      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   1166604      C   ...ads/julia-1.6.3/bin/julia      251MiB |\n",
      "|    1   N/A  N/A      1028      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    1   N/A  N/A      1646      G   /usr/lib/xorg/Xorg                176MiB |\n",
      "|    1   N/A  N/A      1780      G   /usr/bin/gnome-shell               35MiB |\n",
      "|    1   N/A  N/A      2180      G   ...AAAAAAAAA= --shared-files       32MiB |\n",
      "|    1   N/A  N/A    666000      G   ...477455.log --shared-files       16MiB |\n",
      "|    1   N/A  N/A   1038483      C   python                          23851MiB |\n",
      "|    1   N/A  N/A   1107979      G   ...AAAAAAAAA= --shared-files       12MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    ";nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d4a878-47a4-43b6-8f4d-d0c4f464fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA GeForce RTX 3090"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device!(0)\n",
    "device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57499c-ee02-4fe9-913e-426686e77117",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Dataset.jl**\n",
    "**TODO**\n",
    "1. Load data\n",
    "2. Minibatch\n",
    "1. Transforms (Normalize, Scale)\n",
    "2. Augmentation (Random crop, random flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368c12e6-0f32-4d9c-8591-c2f9915e1eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Pkg, Colors, Images, Knet, Statistics, Plots; default(fmt = :png) \n",
    "using Base.Iterators: flatten\n",
    "using IterTools: ncycle, takenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74954d23-a40f-4b9e-b5a6-151e17535287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_image_paths = split(read(`ls -1v /home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Image`, String));\n",
    "train_mask_paths = split(read(`ls -1v /home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Mask`, String));\n",
    "train_contour_paths = split(read(`ls -1v /home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Mask`, String));\n",
    "\n",
    "train_image_paths = [\"/home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Image/\"*i for i in train_image_paths]\n",
    "train_mask_paths = [\"/home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Mask/\"*i for i in train_mask_paths];\n",
    "train_contour_paths = [\"/home/mertcokelek/Desktop/Github/VST/RGB_VST/Data/DUTS/DUTS-TR/DUTS-TR-Mask/\"*i for i in train_contour_paths];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905d3c5-3628-430f-8055-b17c8885330a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3db7b82a-8ad5-47f7-999c-0b494fb6a3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess_img (generic function with 2 methods)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function preprocess_img(instance; imsize=224, random_crop=true, random_flip=true)\n",
    "    new_img = deepcopy(instance[:,:,1]) # img = 0 at the end?\n",
    "    new_msk = deepcopy(instance[:,:,2]) # img = 0 at the end?\n",
    "    new_cnt = deepcopy(instance[:,:,3]) # img = 0 at the end?\n",
    "    if random_flip\n",
    "        if rand() > 0.5\n",
    "            new_img = reverse(new_img, dims=2)\n",
    "            new_msk = reverse(new_msk, dims=2)\n",
    "            new_cnt = reverse(new_cnt, dims=2)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if random_crop\n",
    "        scale_size = 256\n",
    "        x1 = rand(1: scale_size-imsize)\n",
    "        y1 = rand(1: scale_size-imsize)\n",
    "        new_img = new_img[x1:x1+imsize-1, y1:y1+imsize-1]\n",
    "        new_msk = new_msk[x1:x1+imsize-1, y1:y1+imsize-1]\n",
    "        new_cnt = new_cnt[x1:x1+imsize-1, y1:y1+imsize-1]\n",
    "    end\n",
    "    return new_img, new_msk, new_cnt\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14a0121b-9ac5-4bb5-ad0d-768ef6d7c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_data(img_paths, mask_paths, contour_paths; imsize=224, temp_n = 32)\n",
    "    images = 0\n",
    "    labels = 0\n",
    "    contours = 0\n",
    "    first = true\n",
    "    temp_n_samples = 0\n",
    "    for (path_img, path_msk, path_con) in zip(img_paths, mask_paths, contour_paths)\n",
    "        if temp_n_samples < temp_n\n",
    "            temp_n_samples += 1\n",
    "            img = Images.imresize(load(path_img), 256, 256)\n",
    "            label = Images.imresize(load(path_msk), 256, 256)\n",
    "            contour = Images.imresize(load(path_con), 256, 256)\n",
    "            \n",
    "            if first\n",
    "                images = cat(img, dims=4)\n",
    "                labels = cat(label, dims=3)\n",
    "                contours = cat(label, dims=3)\n",
    "                first = false\n",
    "            else\n",
    "                images = cat(images, img, dims=4)\n",
    "                labels = cat(labels, label, dims=3)\n",
    "                contours = cat(contours, contour, dims=3)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # gt = permutedims(cat(labels, contours, dims=4), [1,2,4,3]) # GT's are concatenated. Permute required for Knet Minibatch\n",
    "    gt = cat(labels, contours, dims=4) # GT's are concatenated. Permute required for Knet Minibatch\n",
    "    return images, permutedims(gt, (1,2,4,3))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34164a02-f99e-455d-8e2a-5ecf7ca9195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_data(train_image_paths, train_mask_paths, train_contour_paths)\n",
    "x_train, y_train = dataset_train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12a18a9a-798b-4bbe-b050-85c6367a1560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 256, 1, 32), (256, 256, 2, 32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(x_train), size(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e767ee2-ce3b-47a0-bf51-bf57026e48bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Knet.Train20.Data{Tuple{Array{RGB{N0f8}, N} where N, Array{Gray{N0f8}, N} where N}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "train_data = Knet.minibatch(x_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "343514b4-cbd2-49ee-960e-1dd02a38ccb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 256, 4), (256, 256, 4), (256, 256, 4))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = first(train_data);\n",
    "imgs = x[:,:,1,:]\n",
    "msks = y[:,:,1,:]\n",
    "cnts = y[:,:,2,:]\n",
    "size(imgs), size(msks), size(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f046680-6d26-4dba-8e38-1a42d2515169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "(3, 224, 224)\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "(3, 224, 224)\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "(3, 224, 224)\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in 1:batch_size\n",
    "    sample, mask, cont = preprocess_img.(eachslice(cat(imgs, msks, cnts, dims=4), dims=3))[i];   \n",
    "    mask = channelview(Gray.(mask)) # Convert gt to grayscale\n",
    "    cont = channelview(Gray.(cont)) # Convert gt to grayscale\n",
    "    \n",
    "    println(size(channelview(sample)))\n",
    "    println(size(mask))\n",
    "    println(size(cont))\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878eb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fa527-dfd3-40ca-b3d4-4565aa1d9973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Knet.Ops21:gelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabcbcf-5681-4bda-b632-379865cbbb6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## token_transformer\n",
    "**Attention, Token_transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4ef6-05c5-444c-9111-ad903b8dee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Attention\n",
    "    dim; num_heads; in_dim; qkv_bias; qk_scale; attn_drop; proj_drop;\n",
    "    \n",
    "    scale; qkv; proj;\n",
    "    function Attention(;dim=784, num_heads=8, in_dim=0, qkv_bias=false, qk_scale=false, attn_drop=0., proj_drop=0.)\n",
    "        self = new(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        head_dim = dim ÷ num_heads\n",
    "        \n",
    "        self.scale = qk_scale || head_dim^-0.5\n",
    "        \n",
    "        self.qkv = Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = Dropout(attn_drop)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "function(self::Attention)(x)\n",
    "    B, N, C = size(x)\n",
    "    qkv = permutedims(reshape(self.qkv(x), (B, N, 3, self.num_heads, self.in_dim)), (3,1,4,2,5))\n",
    "    q, k, v = qkv[1], qkv[1], qkv[2]\n",
    "    \n",
    "    attn = (q * transposedims(k, -2, -1)) .* self.scale\n",
    "    softmax_dim = length(size(attn))\n",
    "    attn = softmax(attn, dims=softmax_dim)\n",
    "    attn = self.attn_drop(attn)\n",
    "    \n",
    "    x = reshape(transposedims(attn * v, 1, 2), (B, N, self.in_dim))\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x, self.proj_drop)\n",
    "    return x\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591312a-74ba-4f6f-8df7-d8286324c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Token_transformer\n",
    "    dim; in_dim; num_heads; mlp_ratio; qkv_bias; qk_scale; drop; attn_drop; drop_path; act_layer; norm_layer;\n",
    "    \n",
    "    norm1; attn; norm2; mlp;\n",
    "    \n",
    "    function Token_transformer(dim, in_dim, num_heads; mlp_ratio=1., qkv_bias=false, qk_scale=false, drop=0., attn_drop=0., drop_path=0., proj_drop=0., act_layer=\"gelu\", norm_layer=\"LayerNorm\")\n",
    "        self = new(dim, in_dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer, norm_layer)\n",
    "\n",
    "        self.norm1=LayerNorm(dim)\n",
    "        self.attn = Attention(;dim=dim, in_dim=in_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "        # self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = LayerNorm(in_dim)\n",
    "        self.mlp = Mlp(in_dim;hidden_features=convert(Int32, in_dim*mlp_ratio), out_features=in_dim, act_layer=gelu, drop=drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function (self::Token_transformer)(x)\n",
    "    x = self.attn(self.norm1(x))\n",
    "    x = x + self.mlp(self.norm2(x))\n",
    "    # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcde7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **transformer_block.jl**\n",
    "**get_sinusoid_encoding, Mlp, Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c27e7-7874-4154-8553-8197491427a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### get_sinusoid_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: atype\n",
    "struct get_sinusoid_encoding \n",
    "    w \n",
    "end\n",
    "\n",
    "function get_sinusoid_encoding(n_position, d_hid; λ=10000, atype=atype())\n",
    "    x = exp.((0:2:d_hid-1) .* -(log(λ)/d_hid)) * (0:n_position-1)'\n",
    "    pe = zeros(d_hid, n_position)\n",
    "    pe[1:2:end,:] = sin.(x)\n",
    "    pe[2:2:end,:] = cos.(x)\n",
    "    get_sinusoid_encoding(atype(pe))\n",
    "end\n",
    "\n",
    "function (l::get_sinusoid_encoding)(x)\n",
    "    x .+ l.w[:,1:size(x,2)]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e24543-10be-44b0-96e5-637534e7a737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71dda1-eaee-4eb5-86e5-42665ac94aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Knet.Layers21: dropout\n",
    "using Knet.Ops21: dropout\n",
    "\n",
    "struct Dropout; p; end\n",
    "\n",
    "function (l::Dropout)(x)        \n",
    "    dropout(x, l.p)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Mlp\n",
    "    in_features; hidden_features; out_features; act_layer; drop;\n",
    "    \n",
    "    fc1; act; fc2;\n",
    "    function Mlp(in_features; hidden_features=nothing, out_features=nothing, act_layer::Function=gelu, drop=0.)\n",
    "        self = new(in_features, hidden_features, out_features, act_layer, drop)\n",
    "        \n",
    "        out_features = out_features == nothing ? in_features : out_features\n",
    "        hidden_features = hidden_features == nothing ? in_features : hidden_features\n",
    "        \n",
    "        self.fc1 = Linear(in_features, hidden_features)\n",
    "        self.act = gelu#() ????\n",
    "        self.fc2 = Linear(hidden_features, out_features)\n",
    "        # self.drop = drop # called in forward pass\n",
    "        self.drop = Dropout(drop)\n",
    "        return self\n",
    "    end\n",
    "    \n",
    "    function (self::Mlp)(x)\n",
    "        x = self.drop(self.act(self.fc1(x)), drop)\n",
    "        x = self.drop(self.fc2(x), drop)\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d93d71-68fb-4c4c-93c2-e52ce46dba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d4a19-7021-4a43-811a-ae79fe1257ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function transposedims(x, dim1, dim2)\n",
    "    \"\"\"\n",
    "    Helper function equivalent to torch.Tensor.transpose(dim1, dim2)\n",
    "    \n",
    "    \"\"\"\n",
    "    size_ = [i for i in size(x)]\n",
    "    dims = length(size_)\n",
    "    dim1_ = dim1 > 0 ? dim1 : dims+dim1+1\n",
    "    dim2_ = dim2 > 0 ? dim2 : dims+dim2+1\n",
    "    \n",
    "    size_[dim1_] = dim2_\n",
    "    size_[dim2_] = dim1_\n",
    "    \n",
    "    return permutedims(x, size_)\n",
    "end\n",
    "\n",
    "# mutable struct Attention\n",
    "#### This is token performer attention\n",
    "#     dim; num_heads; qkv_bias; qk_scale; attn_drop; proj_drop;\n",
    "    \n",
    "#     scale; qkv; proj;\n",
    "#     function Attention(;dim=784, num_heads=8, qkv_bias=false, qk_scale=false, attn_drop=0., proj_drop=0.)\n",
    "#         self = new(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
    "#         self.num_heads = num_heads\n",
    "#         head_dim = dim ÷ num_heads\n",
    "        \n",
    "#         self.scale = qk_scale || head_dim^-0.5\n",
    "        \n",
    "#         self.qkv = Linear(dim, dim*3, bias=qkv_bias)\n",
    "#         self.attn_drop = Dropout(attn_drop)\n",
    "#         self.proj = Linear(dim, dim)\n",
    "#         self.proj_drop = Dropout(proj_drop)\n",
    "#         return self\n",
    "#     end\n",
    "# end\n",
    "# function(self::Attention)(x)\n",
    "#     B, N, C = size(x)\n",
    "#     qkv = permutedims(reshape(self.qkv(x), (B, N, 3, self.num_heads, C ÷ self.num_heads)), (3,1,4,2,5))\n",
    "#     q, k, v = qkv[1], qkv[1], qkv[2]\n",
    "    \n",
    "#     attn = (q * transposedims(k, -2, -1)) .* self.scale\n",
    "#     softmax_dim = length(size(attn))\n",
    "#     attn = softmax(attn, dims=softmax_dim)\n",
    "#     attn = self.attn_drop(attn)\n",
    "    \n",
    "#     x = reshape(transposedims(attn * v, 1, 2), (B, N, C))\n",
    "#     x = self.proj(x)\n",
    "#     x = self.proj_drop(x, self.proj_drop)\n",
    "#     return x\n",
    "# end \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9f02e-b4e8-4c86-b0bc-6e52772527a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab6eb2-6b74-43e1-8885-3e1332e1582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Block\n",
    "    dim; num_heads; mlp_ratio; qkv_bias; qk_scale; drop; attn_drop;\n",
    "    drop_path; act_layer; norm_layer;\n",
    "    \n",
    "    norm1;\n",
    "    attn;\n",
    "    norm2;\n",
    "    mlp;\n",
    "    \n",
    "    function Block(;dim=784, num_heads=8, mlp_ratio=4.0, qkv_bias=false, qk_scale=false, drop=0., attn_drop=0.,\n",
    "        drop_path=0., act_layer::Function=gelu, norm_layer=\"LayerNorm\")\n",
    "    \n",
    "        self = new(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer, norm_layer)\n",
    "        \n",
    "        @assert norm_layer == \"LayerNorm\"\n",
    "        self.norm1 = LayerNorm(dim)\n",
    "        \n",
    "        self.attn =  Attention(dim=dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        \n",
    "        # TODO: DropPath: If drop_path != 0., Adapt https://github.com/rwightman/pytorch-image-models/blob/f7d210d759beb00a3d0834a3ce2d93f6e17f3d38/timm/models/layers/drop.py#L160 to Julia\n",
    "        # self.drop_path = drop_path > 0.? DropPath(drop_path) : nn.Identity()  # ????\n",
    "                \n",
    "        self.norm2 = LayerNorm(dim)\n",
    "        mlp_hidden_dim = convert(Int, dim * mlp_ratio)\n",
    "        self.mlp = Mlp(dim; hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "function (self::Block)(x)\n",
    "    # x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "    # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "    \n",
    "    x = x + self.attn(self.norm1(x))\n",
    "    x = x + self.mlp(self.norm2(x))\n",
    "    return x\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad56303-8e19-4161-9cb7-32a6eb6fac00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **token_performer.jl**\n",
    "**Token_performer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410a3da-bcc9-42c8-8bf0-ac385d9a2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Token_performer\n",
    "    dim; in_dim; head_cnt; kernel_ratio; dp1; dp2;\n",
    "    \n",
    "    emb; kqv; dp; proj; norm1; norm2; epsilon; mlp; m; w;\n",
    "    \n",
    "    function Token_performer(dim, in_dim; head_cnt=1, kernel_ratio=0.5, dp1=0.1, dp2=0.1)\n",
    "        self = new(dim, in_dim, head_cnt, kernel_ratio, dp1, dp2)\n",
    "        self.emb = in_dim * head_cnt\n",
    "        self.kqv = Linear(dim, 3*self.emb)\n",
    "        self.dp = Dropout(dp1)\n",
    "        # self.dp = dp1\n",
    "        self.proj = Linear(self.emb, self.emb)\n",
    "        self.head_cnt = head_cnt\n",
    "        # self.norm1 = LayerNorm(dim) # forward pass\n",
    "        # self.norm2 = LayerNorm(self.emb) \"\n",
    "\n",
    "        # TODO: gelu\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.emb, 1*self.emb),\n",
    "            gelu,\n",
    "            Linear(1*self.emb, self.emb),\n",
    "            Dropout(dp2)\n",
    "        )\n",
    "        \n",
    "        self.m = convert(Int32, self.emb * kernel_ratio)\n",
    "        self.w = rand(self.m, self.emb)\n",
    "        # TODO: Initialization method\n",
    "        self.w = convert(KnetArray, self.w)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function prm_exp(self, x)\n",
    "    # x = (B, T, hs)\n",
    "    # w = (m, hs)\n",
    "    # return : x : B, T, m\n",
    "    # SM(x, y) = E_w[exp(w^T x - |x|/2) exp(w^T y - |y|/2)]\n",
    "    # therefore return exp(w^Tx - |x|/2)/sqrt(m)\n",
    "    \n",
    "    axis = length(size(x))\n",
    "    xd = repeat(sum(x .* x, dims=axis), 1,1,self.m) ./ 2\n",
    "    wtx = zeros(size(xd))\n",
    "    @einsum wtx[b,t,m] = x[b,t,i] * w[m, i]\n",
    "    return exp.(wtx - xd) / sqrt(self.m)\n",
    "end\n",
    "\n",
    "function single_attn(self, x)\n",
    "    kqv_out = reshape(self.kqv(x), 3, self.emb)\n",
    "    k, q, v = kqv_out[1,:], kqv_out[2,:], kqv_out[3,:]\n",
    "    kq, qp = self.prm_exp(k), self.prm_exp(q) # B, T, m\n",
    "    B, T, m = size(kq)\n",
    "    D = zeros((B, T))\n",
    "    kp_sum = sum(kp, dims=2)\n",
    "    @einsum D[b,t] = qp[b,t,i] * kp_sum[b,i]\n",
    "    D = reshape(D, (B, T, 1)) # (B, T, m) * (B, m) -> (B, T, 1)\n",
    "    kptv = zeros((B, self.emb, m)); @einsum kptv[b,n,m] = v[b,i,n] * kp[b,i,m];\n",
    "    y = zeros((B, T, self.emb));\n",
    "    @einsum y[b,t,n] = qp[b,t,i] * kptv[b,n,i]\n",
    "    y /= (repeat(D, 1, 1, self.emb) + self.epsilon)  # (B, T, emb)*Diag\n",
    "    #skip connection\n",
    "    # y = v + self.dp(self.proj(y))  # same as token_transformer in T2T layer, use v as skip connection\n",
    "    y = self.dp(self.proj(y))\n",
    "    return y\n",
    "end\n",
    "\n",
    "function (self::Token_performer)(x)\n",
    "    x += self.single_attn(LayerNorm(x))\n",
    "    x += self.mlp(LayerNorm(x))\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ada5a-88b5-4eda-80ae-8ae2570b6edf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **utils.jl**\n",
    "**Linear, \n",
    "LayerNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91802a8c-3439-401d-8ed7-0a2eb7cc7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LayerNorm; a; b; ϵ; end\n",
    "\n",
    "function LayerNorm(dmodel; eps=1e-6)\n",
    "    a = param(dmodel; init=ones)\n",
    "    b = param(dmodel; init=zeros)\n",
    "    LayerNorm(a, b, eps)\n",
    "end\n",
    "\n",
    "function (l::LayerNorm)(x, o...)\n",
    "    μ = mean(x,dims=1)\n",
    "    σ = std(x,mean=μ,dims=1)\n",
    "    ϵ = eltype(x)(l.ϵ)\n",
    "    l.a .* (x .- μ) ./ (σ .+ ϵ) .+ l.b # To Do: doing x .- μ twice?\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Linear; w; b; end\n",
    "\n",
    "function Linear(input::Int,outputs...; bias=true)\n",
    "    Linear(param(outputs...,input), bias ? param0(outputs...) : nothing)\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    W1,W2,X1,X2 = size(l.w)[1:end-1], size(l.w)[end], size(x,1), size(x)[2:end]; \n",
    "    @assert W2===X1\n",
    "    y = reshape(l.w,:,W2) * reshape(x,X1,:)\n",
    "    y = reshape(y, W1..., X2...)\n",
    "    if l.b !== nothing; y = y .+ l.b; end\n",
    "    return y\n",
    "end\n",
    "\n",
    "# Chain\n",
    "mutable struct Sequential; layers; end\n",
    "\n",
    "function Sequential(layer1, layer2, layers...)\n",
    "    Sequential((layer1, layer2, layers...))\n",
    "end\n",
    "\n",
    "function (l::Sequential)(x, o...)\n",
    "    for layer in l.layers\n",
    "        x = layer(x, o...)\n",
    "    end\n",
    "    return x\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ace3c2-64e9-4e79-9e49-cbaf4c779eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **t2t_vit.jl**\n",
    "**T2T_module, T2T_ViT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021b011-15be-4017-96c5-b0fdc7927f82",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T2T_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772d498-00d7-4907-aa61-ede6de803151",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct T2T_module\n",
    "    \"\"\"\n",
    "    Tokens-to-Token encoding module\n",
    "    \"\"\"\n",
    "    img_size; tokens_type;\n",
    "    in_chans\n",
    "    embed_dim; token_dim;\n",
    "    \n",
    "    \n",
    "    soft_split0; soft_split1; soft_split2\n",
    "    attention1; attention2\n",
    "    project\n",
    "    num_patches\n",
    "    \n",
    "    function T2T_module(img_size=224, \n",
    "                        tokens_type=\"transformer\", \n",
    "                        in_chans=3, \n",
    "                        embed_dim=768, \n",
    "                        token_dim=64)\n",
    "        self = new(img_size, tokens_type, in_chans, embed_dim, token_dim)\n",
    "        \n",
    "        if tokens_type == \"transformer\"\n",
    "            println(\"adopt transformer encoder for tokens-to-token\")\n",
    "            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
    "            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            \n",
    "            # self.attention1 = Token_performer(in_chans*7*7, token_dim; kernel_ratio=0.5)\n",
    "            # self.attention2 = Token_performer(token_dim*3*3, token_dim; kernel_ratio=0.5)\n",
    "\n",
    "            self.attention1 = Token_transformer(in_chans * 7 * 7, token_dim, 1; mlp_ratio=1.0)\n",
    "            self.attention2 = Token_transformer(token_dim * 3 * 3, token_dim, 1; mlp_ratio=1.0)\n",
    "            self.project = Linear(token_dim * 3 * 3, embed_dim)\n",
    "        end\n",
    "        \n",
    "        self.num_patches = (img_size ÷ (4 * 2 * 2)) * (img_size ÷ (4 * 2 * 2))  # there are 3 soft split, stride are 4,2,2 seperately\n",
    "\n",
    "        return self\n",
    "        \"\"\"\n",
    "        TODO / NOTE: initially, let's just consider 'performer' model. 'transformer' & 'convolution' can be done later.\n",
    "        TODO: Unfold currently taken from PyTorch. Might need conversion between datatypes.\n",
    "        \"\"\"\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function (self::T2T_module)(x)\n",
    "    # step0: soft split\n",
    "    x = permutedims(self.soft_split0(x), [2, 3])\n",
    "    \n",
    "    # x [B, 56*56, 147=7*7*3]\n",
    "    # iteration1: restructurization/reconstruction\n",
    "    \n",
    "    x_1_4 = self.attention1(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_4, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split1(x), [2, 3])\n",
    "\n",
    "    # iteration2: restructurization/reconstruction\n",
    "    x_1_8 = self.attention2(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_8, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split2(x), [2, 3])\n",
    "\n",
    "    # final_tokens\n",
    "    x = self.project(x)\n",
    "    \n",
    "    return x, x_1_8, x_1_4\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60bc55-1c42-48b1-b695-1608d583b7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T2T_ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43185d-8d84-4f21-b8df-963f04a7b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct T2T_ViT; \n",
    "    img_size; tokens_type\n",
    "    in_chans; num_classes; num_features\n",
    "    embed_dim; depth\n",
    "    num_heads; mlp_ratio\n",
    "    qkv_bias; qk_scale\n",
    "    drop_rate; attn_drop_rate; drop_path_rate; \n",
    "    norm_layer;\n",
    "\n",
    "    blocks\n",
    "    norm\n",
    "    head\n",
    "    tokens_to_token\n",
    "    cls_token\n",
    "    pos_embed\n",
    "\n",
    "    function T2T_ViT(;img_size=224, \n",
    "            tokens_type=\"transformer\", \n",
    "            in_chans=3, num_classes=1000,\n",
    "            embed_dim=784,depth=12,\n",
    "            num_heads=12, mlp_ratio=4., \n",
    "            qkv_bias=false, qk_scale=false, \n",
    "            drop_rate=0., attn_drop_rate=0., drop_path_rate=0., \n",
    "            norm_layer=\"LayerNorm\"\n",
    "            )\n",
    "            \n",
    "        self = new(img_size, tokens_type, \n",
    "                    in_chans, num_classes, \n",
    "                    embed_dim,depth,num_heads,\n",
    "                    mlp_ratio,qkv_bias,qk_scale,\n",
    "                    drop_rate,attn_drop_rate,\n",
    "                    drop_path_rate,norm_layer)\n",
    "        self.num_features = embed_dim\n",
    "\n",
    "        self.tokens_to_token = T2T_module()\n",
    "        self.cls_token = convert(KnetArray, zeros(1,1,768))\n",
    "        self.pos_embed = reshape(get_sinusoid_encoding(16, 784).w, (1, 16, 784))  #\n",
    "        \n",
    "\n",
    "        # NOTE: In PyTorch implementation, dropout is added as a portable layer.\n",
    "        # We will add it as an operation, in the forward pass. \n",
    "        # UPDATE: probably we won't need it, since we'll use pretrained weights. \n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        dpr = [i for i in 0:0.25/depth:0.25] # Stochastic depth decay rule\n",
    "        self.blocks = [Block(dim=embed_dim, \n",
    "                            num_heads=num_heads, \n",
    "                            mlp_ratio=mlp_ratio, \n",
    "                            qkv_bias=qkv_bias, \n",
    "                            qk_scale=qk_scale,\n",
    "                            drop=drop_rate, \n",
    "                            attn_drop=attn_drop_rate,\n",
    "                            drop_path=dpr[i], \n",
    "                            act_layer=gelu,\n",
    "                            norm_layer=norm_layer) for i in 1:depth]\n",
    "        @assert norm_layer == \"LayerNorm\"                   \n",
    "        self.norm = LayerNorm(embed_dim)\n",
    "        # Classifier head\n",
    "        self.head = Linear(embed_dim, num_classes)\n",
    "#         For initializing weights, probably we won't need the rest.\n",
    "#         We will use pretrained weights.\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function expand(x, B)\n",
    "    \"\"\"\n",
    "    Helper function for torch.Tensor.expand()    expand(x, B) is equivalent to x.expand(B, -1, -1)\n",
    "    \"\"\"\n",
    "    res = x\n",
    "    dimx = length(size(x))\n",
    "    for i in 1:B-1\n",
    "        res = cat(res, x, dims=dimx+1)\n",
    "    end\n",
    "    permute_order = [i for i in 1:dimx]\n",
    "    insert!(permute_order, 1, dimx+1)\n",
    "    res = permutedims(res, permute_order)\n",
    "end\n",
    "\n",
    "function (self::T2T_ViT)(x)\n",
    "    B = size(x)[1]\n",
    "    x, x_1_8, x_1_4 = self.tokens_to_token(x)\n",
    "    \n",
    "    cls_tokens = expand(self.cls_token, B) # self.cls_token.expand(B, -1, -1) in Torch\n",
    "    x = hcat(cls_tokens, x)\n",
    "    x += self.pos_embed\n",
    "    # x = self.pos_drop # Dropout will not be used for now.\n",
    "    \n",
    "    # T2T-ViT backbone\n",
    "    for blk in self.blocks\n",
    "        x = blk(x)\n",
    "    end\n",
    "    \n",
    "    x = self.norm(x)\n",
    "    return x[:,2:end,:], x_1_8, x_1_4\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ca7b5-e414-4f62-be41-0babcb5dcef2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Load Pretrained Weights for RGB-Encoder T2T_ViT-14 Backbone**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d748005-6f93-4de1-8566-fd054a92206a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **loadPretrainedWeightsT2TViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c6e38-17d5-43df-8361-53b0b29ce3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/mertcokelek/Downloads/80.7_T2T_ViT_t_14.pth.tar\"\n",
    "\n",
    "function print_pretrained(path)\n",
    "    weights = torch.load(path, map_location=torch.device(\"cpu\"))[\"state_dict_ema\"];\n",
    "    weights = sort(collect(weights), by = x->x[1])\n",
    "\n",
    "    for (k, v) in weights\n",
    "        @show k, v.shape\n",
    "    end\n",
    "end\n",
    "\n",
    "function loadPretrainedWeightsT2TViT(path, model)\n",
    "    weights = torch.load(path, map_location=torch.device(\"cpu\"))[\"state_dict_ema\"];\n",
    "    for i in 1:14\n",
    "        # blocks\n",
    "        model.blocks[i].attn.qkv.w = Param(atype(weights[\"blocks.$(i-1).attn.qkv.weight\"][:cpu]()[:numpy]()))\n",
    "        \n",
    "        model.blocks[i].attn.proj.w = Param(atype(weights[\"blocks.$(i-1).attn.proj.weight\"][:cpu]()[:numpy]()))\n",
    "        model.blocks[i].attn.proj.b = Param(atype(weights[\"blocks.$(i-1).attn.proj.bias\"][:cpu]()[:numpy]()))\n",
    "        \n",
    "        model.blocks[i].mlp.fc1.w = Param(atype(weights[\"blocks.$(i-1).mlp.fc1.weight\"][:cpu]()[:numpy]()))\n",
    "        model.blocks[i].mlp.fc1.b = Param(atype(weights[\"blocks.$(i-1).mlp.fc1.bias\"][:cpu]()[:numpy]()))\n",
    "        \n",
    "        \n",
    "        model.blocks[i].mlp.fc2.w = Param(atype(weights[\"blocks.$(i-1).mlp.fc2.weight\"][:cpu]()[:numpy]()))\n",
    "        model.blocks[i].mlp.fc2.b = Param(atype(weights[\"blocks.$(i-1).mlp.fc2.bias\"][:cpu]()[:numpy]()))\n",
    "        \n",
    "        model.blocks[i].norm1.a = Param(atype(weights[\"blocks.$(i-1).norm1.weight\"][:cpu]()[:numpy]()))\n",
    "        model.blocks[i].norm1.b = Param(atype(weights[\"blocks.$(i-1).norm1.bias\"][:cpu]()[:numpy]()))\n",
    "        \n",
    "        model.blocks[i].norm2.a = Param(atype(weights[\"blocks.$(i-1).norm2.weight\"][:cpu]()[:numpy]()))\n",
    "        model.blocks[i].norm2.b = Param(atype(weights[\"blocks.$(i-1).norm2.bias\"][:cpu]()[:numpy]()))\n",
    "   end\n",
    "\n",
    "    # tokens_to_token\n",
    "          # attn 1\n",
    "    model.tokens_to_token.attention1.attn.proj.w = Param(atype(weights[\"tokens_to_token.attention1.attn.proj.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.attn.proj.b = Param(atype(weights[\"tokens_to_token.attention1.attn.proj.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention1.attn.qkv.w = Param(atype(weights[\"tokens_to_token.attention1.attn.qkv.weight\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention1.mlp.fc1.w = Param(atype(weights[\"tokens_to_token.attention1.mlp.fc1.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.mlp.fc1.b = Param(atype(weights[\"tokens_to_token.attention1.mlp.fc1.bias\"][:cpu]()[:numpy]()))\n",
    "   \n",
    "    model.tokens_to_token.attention1.mlp.fc2.w = Param(atype(weights[\"tokens_to_token.attention1.mlp.fc2.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.mlp.fc2.b = Param(atype(weights[\"tokens_to_token.attention1.mlp.fc2.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention1.norm1.a = Param(atype(weights[\"tokens_to_token.attention1.norm1.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.norm1.b = Param(atype(weights[\"tokens_to_token.attention1.norm1.bias\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.norm2.a = Param(atype(weights[\"tokens_to_token.attention1.norm2.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention1.norm2.b = Param(atype(weights[\"tokens_to_token.attention1.norm2.bias\"][:cpu]()[:numpy]()))\n",
    "    \n",
    "          # attn2\n",
    "    model.tokens_to_token.attention2.attn.proj.w = Param(atype(weights[\"tokens_to_token.attention2.attn.proj.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.attn.proj.b = Param(atype(weights[\"tokens_to_token.attention2.attn.proj.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention2.attn.qkv.w = Param(atype(weights[\"tokens_to_token.attention2.attn.qkv.weight\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention2.mlp.fc1.w = Param(atype(weights[\"tokens_to_token.attention2.mlp.fc1.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.mlp.fc1.b = Param(atype(weights[\"tokens_to_token.attention2.mlp.fc1.bias\"][:cpu]()[:numpy]()))\n",
    "   \n",
    "    model.tokens_to_token.attention2.mlp.fc2.w = Param(atype(weights[\"tokens_to_token.attention2.mlp.fc2.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.mlp.fc2.b = Param(atype(weights[\"tokens_to_token.attention2.mlp.fc2.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.tokens_to_token.attention2.norm1.a = Param(atype(weights[\"tokens_to_token.attention2.norm1.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.norm1.b = Param(atype(weights[\"tokens_to_token.attention2.norm1.bias\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.norm2.a = Param(atype(weights[\"tokens_to_token.attention2.norm2.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.attention2.norm2.b = Param(atype(weights[\"tokens_to_token.attention2.norm2.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "         # project\n",
    "    model.tokens_to_token.project.w = Param(atype(weights[\"tokens_to_token.project.weight\"][:cpu]()[:numpy]()))\n",
    "    model.tokens_to_token.project.b = Param(atype(weights[\"tokens_to_token.project.bias\"][:cpu]()[:numpy]()))\n",
    "    \n",
    "    \n",
    "    model.cls_token = Param(atype(weights[\"cls_token\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.head.w = Param(atype(weights[\"head.weight\"][:cpu]()[:numpy]()))\n",
    "    model.head.b = Param(atype(weights[\"head.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.norm.a = Param(atype(weights[\"norm.weight\"][:cpu]()[:numpy]()))\n",
    "    model.norm.b = Param(atype(weights[\"norm.bias\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    model.pos_embed = Param(atype(weights[\"pos_embed\"][:cpu]()[:numpy]()))\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0617d9-cda2-478d-852a-e5d97b25592a",
   "metadata": {},
   "source": [
    "### **Pretrained T2T-ViT Backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c5d30-79fd-4690-9004-6c076710a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/mertcokelek/Downloads/80.7_T2T_ViT_t_14.pth.tar\"\n",
    "rgb_backbone = T2T_ViT(tokens_type=\"transformer\", embed_dim=384, depth=14, num_heads=6, mlp_ratio=3.);\n",
    "model_pt = loadPretrainedWeightsT2TViT(model_path, rgb_backbone);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
