{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9dee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using PyCall\n",
    "using Knet\n",
    "using Knet.CUDA\n",
    "using Knet: atype\n",
    "using Einsum\n",
    "@pyimport torch\n",
    "@pyimport torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558ad121-f5a7-47e7-bd8b-b159829f4baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{CuDevice}:\n",
       " CuDevice(0): NVIDIA GeForce RTX 3090\n",
       " CuDevice(1): NVIDIA GeForce RTX 3090"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c1d3d1-b1e9-48e1-9bd0-0a407e1585f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(1): NVIDIA GeForce RTX 3090"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device!(1)\n",
    "device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878eb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "407fa527-dfd3-40ca-b3d4-4565aa1d9973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Knet.Ops21:gelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcde7ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **transformer_block.jl**\n",
    "**get_sinusoid_encoding, Mlp, Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c27e7-7874-4154-8553-8197491427a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### get_sinusoid_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00f124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: atype\n",
    "struct get_sinusoid_encoding \n",
    "    w \n",
    "end\n",
    "\n",
    "function get_sinusoid_encoding(n_position, d_hid; λ=10000, atype=atype())\n",
    "    x = exp.((0:2:d_hid-1) .* -(log(λ)/d_hid)) * (0:n_position-1)'\n",
    "    pe = zeros(d_hid, n_position)\n",
    "    pe[1:2:end,:] = sin.(x)\n",
    "    pe[2:2:end,:] = cos.(x)\n",
    "    get_sinusoid_encoding(atype(pe))\n",
    "end\n",
    "\n",
    "function (l::get_sinusoid_encoding)(x)\n",
    "    x .+ l.w[:,1:size(x,2)]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e24543-10be-44b0-96e5-637534e7a737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f71dda1-eaee-4eb5-86e5-42665ac94aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Knet.Layers21: dropout\n",
    "using Knet.Ops21: dropout\n",
    "\n",
    "struct Dropout; p; end\n",
    "\n",
    "function (l::Dropout)(x)        \n",
    "    dropout(x, l.p)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Mlp\n",
    "    in_features; hidden_features; out_features; act_layer; drop;\n",
    "    \n",
    "    fc1; act; fc2;\n",
    "    function Mlp(in_features; hidden_features=nothing, out_features=nothing, act_layer::Function=gelu, drop=0.)\n",
    "        self = new(in_features, hidden_features, out_features, act_layer, drop)\n",
    "        \n",
    "        out_features = out_features == nothing ? in_features : out_features\n",
    "        hidden_features = hidden_features == nothing ? in_features : hidden_features\n",
    "        \n",
    "        self.fc1 = Linear(in_features, hidden_features)\n",
    "        self.act = act_layer#() ????\n",
    "        self.fc2 = Linear(hidden_features, out_features)\n",
    "        # self.drop = drop # called in forward pass\n",
    "        self.drop = Dropout(drop)\n",
    "        return self\n",
    "    end\n",
    "    \n",
    "    function (self::Mlp)(x)\n",
    "        x = self.drop(self.act(self.fc1(x)), drop)\n",
    "        x = self.drop(self.fc2(x), drop)\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d93d71-68fb-4c4c-93c2-e52ce46dba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f1d4a19-7021-4a43-811a-ae79fe1257ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Attention\n",
    "    dim; num_heads; qkv_bias; qk_scale; attn_drop; proj_drop;\n",
    "    \n",
    "    scale; qkv; proj;\n",
    "    function Attention(;dim=784, num_heads=8, qkv_bias=false, qk_scale=false, attn_drop=0., proj_drop=0.)\n",
    "        self = new(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim ÷ num_heads\n",
    "        \n",
    "        self.scale = qk_scale || head_dim^-0.5\n",
    "        \n",
    "        self.qkv = Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = Dropout(attn_drop)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function transposedims(x, dim1, dim2)\n",
    "    \"\"\"\n",
    "    Helper function equivalent to torch.Tensor.transpose(dim1, dim2)\n",
    "    \n",
    "    \"\"\"\n",
    "    size_ = [i for i in size(x)]\n",
    "    dims = length(size_)\n",
    "    dim1_ = dim1 > 0 ? dim1 : dims+dim1+1\n",
    "    dim2_ = dim2 > 0 ? dim2 : dims+dim2+1\n",
    "    \n",
    "    size_[dim1_] = dim2_\n",
    "    size_[dim2_] = dim1_\n",
    "    \n",
    "    return permutedims(x, size_)\n",
    "end\n",
    "\n",
    "function(self::Attention)(x)\n",
    "    B, N, C = size(x)\n",
    "    qkv = permutedims(reshape(self.qkv(x), (B, N, 3, self.num_heads, C ÷ self.num_heads)), (3,1,4,2,5))\n",
    "    q, k, v = qkv[1], qkv[1], qkv[2]\n",
    "    \n",
    "    attn = (q * transposedims(k, -2, -1)) .* self.scale\n",
    "    softmax_dim = length(size(attn))\n",
    "    attn = softmax(attn, dims=softmax_dim)\n",
    "    attn = self.attn_drop(attn)\n",
    "    \n",
    "    x = reshape(transposedims(attn * v, 1, 2), (B, N, C))\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x, self.proj_drop)\n",
    "    return x\n",
    "end \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9f02e-b4e8-4c86-b0bc-6e52772527a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15ab6eb2-6b74-43e1-8885-3e1332e1582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Block\n",
    "    dim; num_heads; mlp_ratio; qkv_bias; qk_scale; drop; attn_drop;\n",
    "    drop_path; act_layer; norm_layer;\n",
    "    \n",
    "    norm1;\n",
    "    attn;\n",
    "    norm2;\n",
    "    mlp;\n",
    "    \n",
    "    function Block(;dim=784, num_heads=8, mlp_ratio=4.0, qkv_bias=false, qk_scale=nothing, drop=0., attn_drop=0.,\n",
    "        drop_path=0., act_layer::Function=gelu, norm_layer=\"LayerNorm\")\n",
    "    \n",
    "        self = new(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer, norm_layer)\n",
    "        \n",
    "        @assert norm_layer == \"LayerNorm\"\n",
    "        self.norm1 = LayerNorm(dim)\n",
    "        \n",
    "        self.attn =  Attention(dim=dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        \n",
    "        # TODO: DropPath: If drop_path != 0., Adapt https://github.com/rwightman/pytorch-image-models/blob/f7d210d759beb00a3d0834a3ce2d93f6e17f3d38/timm/models/layers/drop.py#L160 to Julia\n",
    "        # self.drop_path = drop_path > 0.? DropPath(drop_path) : nn.Identity()  # ????\n",
    "                \n",
    "        self.norm2 = LayerNorm(dim)\n",
    "        mlp_hidden_dim = convert(Int, dim * mlp_ratio)\n",
    "        self.mlp = Mlp(dim; hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "function (self::Block)(x)\n",
    "    # x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "    # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "    \n",
    "    x = x + self.attn(self.norm1(x))\n",
    "    x = x + self.mlp(self.norm2(x))\n",
    "    return x\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad56303-8e19-4161-9cb7-32a6eb6fac00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **token_performer.jl**\n",
    "**Token_performer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5410a3da-bcc9-42c8-8bf0-ac385d9a2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Token_performer\n",
    "    dim; in_dim; head_cnt; kernel_ratio; dp1; dp2;\n",
    "    \n",
    "    emb; kqv; dp; proj; norm1; norm2; epsilon; mlp; m; w;\n",
    "    \n",
    "    function Token_performer(dim, in_dim; head_cnt=1, kernel_ratio=0.5, dp1=0.1, dp2=0.1)\n",
    "        self = new(dim, in_dim, head_cnt, kernel_ratio, dp1, dp2)\n",
    "        self.emb = in_dim * head_cnt\n",
    "        self.kqv = Linear(dim, 3*self.emb)\n",
    "        self.dp = Dropout(dp1)\n",
    "        # self.dp = dp1\n",
    "        self.proj = Linear(self.emb, self.emb)\n",
    "        self.head_cnt = head_cnt\n",
    "        # self.norm1 = LayerNorm(dim) # forward pass\n",
    "        # self.norm2 = LayerNorm(self.emb) \"\n",
    "\n",
    "        # TODO: gelu\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.emb, 1*self.emb),\n",
    "            gelu,\n",
    "            Linear(1*self.emb, self.emb),\n",
    "            Dropout(dp2)\n",
    "        )\n",
    "        \n",
    "        self.m = convert(Int32, self.emb * kernel_ratio)\n",
    "        self.w = rand(self.m, self.emb)\n",
    "        # TODO: Initialization method\n",
    "        self.w = convert(KnetArray, self.w)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function prm_exp(self, x)\n",
    "    # x = (B, T, hs)\n",
    "    # w = (m, hs)\n",
    "    # return : x : B, T, m\n",
    "    # SM(x, y) = E_w[exp(w^T x - |x|/2) exp(w^T y - |y|/2)]\n",
    "    # therefore return exp(w^Tx - |x|/2)/sqrt(m)\n",
    "    \n",
    "    axis = length(size(x))\n",
    "    xd = repeat(sum(x .* x, dims=axis), 1,1,self.m) ./ 2\n",
    "    wtx = zeros(size(xd))\n",
    "    @einsum wtx[b,t,m] = x[b,t,i] * w[m, i]\n",
    "    return exp.(wtx - xd) / sqrt(self.m)\n",
    "end\n",
    "\n",
    "function single_attn(self, x)\n",
    "    kqv_out = reshape(self.kqv(x), 3, self.emb)\n",
    "    k, q, v = kqv_out[1,:], kqv_out[2,:], kqv_out[3,:]\n",
    "    kq, qp = self.prm_exp(k), self.prm_exp(q) # B, T, m\n",
    "    B, T, m = size(kq)\n",
    "    D = zeros((B, T))\n",
    "    kp_sum = sum(kp, dims=2)\n",
    "    @einsum D[b,t] = qp[b,t,i] * kp_sum[b,i]\n",
    "    D = reshape(D, (B, T, 1)) # (B, T, m) * (B, m) -> (B, T, 1)\n",
    "    kptv = zeros((B, self.emb, m)); @einsum kptv[b,n,m] = v[b,i,n] * kp[b,i,m];\n",
    "    y = zeros((B, T, self.emb));\n",
    "    @einsum y[b,t,n] = qp[b,t,i] * kptv[b,n,i]\n",
    "    y /= (repeat(D, 1, 1, self.emb) + self.epsilon)  # (B, T, emb)*Diag\n",
    "    #skip connection\n",
    "    # y = v + self.dp(self.proj(y))  # same as token_transformer in T2T layer, use v as skip connection\n",
    "    y = self.dp(self.proj(y))\n",
    "    return y\n",
    "end\n",
    "\n",
    "function (self::Token_performer)(x)\n",
    "    x += self.single_attn(LayerNorm(x))\n",
    "    x += self.mlp(LayerNorm(x))\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ada5a-88b5-4eda-80ae-8ae2570b6edf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **utils.jl**\n",
    "**Linear, \n",
    "LayerNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91802a8c-3439-401d-8ed7-0a2eb7cc7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LayerNorm; a; b; ϵ; end\n",
    "\n",
    "function LayerNorm(dmodel; eps=1e-6)\n",
    "    a = param(dmodel; init=ones)\n",
    "    b = param(dmodel; init=zeros)\n",
    "    LayerNorm(a, b, eps)\n",
    "end\n",
    "\n",
    "function (l::LayerNorm)(x, o...)\n",
    "    μ = mean(x,dims=1)\n",
    "    σ = std(x,mean=μ,dims=1)\n",
    "    ϵ = eltype(x)(l.ϵ)\n",
    "    l.a .* (x .- μ) ./ (σ .+ ϵ) .+ l.b # To Do: doing x .- μ twice?\n",
    "end\n",
    "\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(input::Int,outputs...; bias=true)\n",
    "    Linear(param(outputs...,input), bias ? param0(outputs...) : nothing)\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    W1,W2,X1,X2 = size(l.w)[1:end-1], size(l.w)[end], size(x,1), size(x)[2:end]; \n",
    "    @assert W2===X1\n",
    "    y = reshape(l.w,:,W2) * reshape(x,X1,:)\n",
    "    y = reshape(y, W1..., X2...)\n",
    "    if l.b !== nothing; y = y .+ l.b; end\n",
    "    return y\n",
    "end\n",
    "\n",
    "# Chain\n",
    "struct Sequential; layers; end\n",
    "\n",
    "function Sequential(layer1, layer2, layers...)\n",
    "    Sequential((layer1, layer2, layers...))\n",
    "end\n",
    "\n",
    "function (l::Sequential)(x, o...)\n",
    "    for layer in l.layers\n",
    "        x = layer(x, o...)\n",
    "    end\n",
    "    return x\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ace3c2-64e9-4e79-9e49-cbaf4c779eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **t2t_vit.jl**\n",
    "**T2T_module, T2T_ViT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021b011-15be-4017-96c5-b0fdc7927f82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### T2T_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0772d498-00d7-4907-aa61-ede6de803151",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct T2T_module\n",
    "    \"\"\"\n",
    "    Tokens-to-Token encoding module\n",
    "    \"\"\"\n",
    "    img_size; tokens_type;\n",
    "    in_chans\n",
    "    embed_dim; token_dim;\n",
    "    \n",
    "    \n",
    "    soft_split0; soft_split1; soft_split2\n",
    "    attention1; attention2\n",
    "    project\n",
    "    num_patches\n",
    "    \n",
    "    function T2T_module(img_size=224, \n",
    "                        tokens_type=\"performer\", \n",
    "                        in_chans=3, \n",
    "                        embed_dim=768, \n",
    "                        token_dim=64)\n",
    "        self = new(img_size, tokens_type, in_chans, embed_dim, token_dim)\n",
    "        \n",
    "        if tokens_type == \"performer\"\n",
    "            println(\"adopt performer encoder for tokens-to-token\")\n",
    "            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
    "            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            \n",
    "            self.attention1 = Token_performer(in_chans*7*7, token_dim; kernel_ratio=0.5)\n",
    "            self.attention2 = Token_performer(token_dim*3*3, token_dim; kernel_ratio=0.5)\n",
    "            self.project = Linear(token_dim * 3 * 3, embed_dim)\n",
    "        end\n",
    "        \n",
    "        self.num_patches = (img_size ÷ (4 * 2 * 2)) * (img_size ÷ (4 * 2 * 2))  # there are 3 soft split, stride are 4,2,2 seperately\n",
    "\n",
    "        return self\n",
    "        \"\"\"\n",
    "        TODO / NOTE: initially, let's just consider 'performer' model. 'transformer' & 'convolution' can be done later.\n",
    "        TODO: Unfold currently taken from PyTorch. Might need conversion between datatypes.\n",
    "        \"\"\"\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function (self::T2T_module)(x)\n",
    "    # step0: soft split\n",
    "    x = permutedims(self.soft_split0(x), [2, 3])\n",
    "    \n",
    "    # x [B, 56*56, 147=7*7*3]\n",
    "    # iteration1: restructurization/reconstruction\n",
    "    \n",
    "    x_1_4 = self.attention1(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_4, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split1(x), [2, 3])\n",
    "\n",
    "    # iteration2: restructurization/reconstruction\n",
    "    x_1_8 = self.attention2(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_8, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split2(x), [2, 3])\n",
    "\n",
    "    # final_tokens\n",
    "    x = self.project(x)\n",
    "    \n",
    "    return x, x_1_8, x_1_4\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60bc55-1c42-48b1-b695-1608d583b7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T2T_ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a43185d-8d84-4f21-b8df-963f04a7b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct T2T_ViT; \n",
    "    img_size; tokens_type\n",
    "    in_chans; num_classes; num_features\n",
    "    embed_dim; depth\n",
    "    num_heads; mlp_ratio\n",
    "    qkv_bias; qk_scale\n",
    "    drop_rate; attn_drop_rate; drop_path_rate; \n",
    "    norm_layer;\n",
    "\n",
    "    blocks\n",
    "    norm\n",
    "    head\n",
    "    tokens_to_token\n",
    "    cls_token\n",
    "    pos_embed\n",
    "\n",
    "    function T2T_ViT(;img_size=224, \n",
    "            tokens_type=\"performer\", \n",
    "            in_chans=3, num_classes=1000,\n",
    "            embed_dim=784,depth=12,\n",
    "            num_heads=12, mlp_ratio=4., \n",
    "            qkv_bias=false, qk_scale=false, \n",
    "            drop_rate=0., attn_drop_rate=0., drop_path_rate=0., \n",
    "            norm_layer=\"LayerNorm\"\n",
    "            )\n",
    "            \n",
    "        self = new(img_size, tokens_type, \n",
    "                    in_chans, num_classes, \n",
    "                    embed_dim,depth,num_heads,\n",
    "                    mlp_ratio,qkv_bias,qk_scale,\n",
    "                    drop_rate,attn_drop_rate,\n",
    "                    drop_path_rate,norm_layer)\n",
    "        self.num_features = embed_dim\n",
    "\n",
    "        self.tokens_to_token = T2T_module()\n",
    "        self.cls_token = convert(KnetArray, zeros(1,1,768))\n",
    "        self.pos_embed = reshape(get_sinusoid_encoding(16, 784).w, (1, 16, 784))  #\n",
    "        \n",
    "\n",
    "        # NOTE: In PyTorch implementation, dropout is added as a portable layer.\n",
    "        # We will add it as an operation, in the forward pass. \n",
    "        # UPDATE: probably we won't need it, since we'll use pretrained weights. \n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        dpr = [i for i in 0:0.25/depth:0.25] # Stochastic depth decay rule\n",
    "        self.blocks = [Block(dim=embed_dim, \n",
    "                            num_heads=num_heads, \n",
    "                            mlp_ratio=mlp_ratio, \n",
    "                            qkv_bias=qkv_bias, \n",
    "                            qk_scale=qk_scale,\n",
    "                            drop=drop_rate, \n",
    "                            attn_drop=attn_drop_rate,\n",
    "                            drop_path=dpr[i], \n",
    "                            act_layer=gelu,\n",
    "                            norm_layer=norm_layer) for i in 1:depth]\n",
    "        @assert norm_layer == \"LayerNorm\"                   \n",
    "        self.norm = LayerNorm(embed_dim)\n",
    "        # Classifier head\n",
    "        self.head = Linear(embed_dim, num_classes)\n",
    "#         For initializing weights, probably we won't need the rest.\n",
    "#         We will use pretrained weights.\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function expand(x, B)\n",
    "    \"\"\"\n",
    "    Helper function for torch.Tensor.expand()    expand(x, B) is equivalent to x.expand(B, -1, -1)\n",
    "    \"\"\"\n",
    "    res = x\n",
    "    dimx = length(size(x))\n",
    "    for i in 1:B-1\n",
    "        res = cat(res, x, dims=dimx+1)\n",
    "    end\n",
    "    permute_order = [i for i in 1:dimx]\n",
    "    insert!(permute_order, 1, dimx+1)\n",
    "    res = permutedims(res, permute_order)\n",
    "end\n",
    "\n",
    "function (self::T2T_ViT)(x)\n",
    "    B = size(x)[1]\n",
    "    x, x_1_8, x_1_4 = self.tokens_to_token(x)\n",
    "    \n",
    "    cls_tokens = expand(self.cls_token, B) # self.cls_token.expand(B, -1, -1) in Torch\n",
    "    x = hcat(cls_tokens, x)\n",
    "    x += self.pos_embed\n",
    "    # x = self.pos_drop # Dropout will not be used for now.\n",
    "    \n",
    "    # T2T-ViT backbone\n",
    "    for blk in self.blocks\n",
    "        x = blk(x)\n",
    "    end\n",
    "    \n",
    "    x = self.norm(x)\n",
    "    return x[:,2:end,:], x_1_8, x_1_4\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ca7b5-e414-4f62-be41-0babcb5dcef2",
   "metadata": {},
   "source": [
    "# **Load Pretrained Weights for RGB-Encoder T2T_ViT-14 Backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f89c30-102c-4ab9-a8ca-64437d984048",
   "metadata": {},
   "outputs": [],
   "source": [
    "function initPretrainedAlbertForMLM!(model::AlbertMLMHead, weights::Dict; atype=atype())\n",
    "    model.projection_layer.w = Param(atype(weights[\"predictions.dense.weight\"][:cpu]()[:numpy]()))\n",
    "    model.projection_layer.b = Param(atype(weights[\"predictions.dense.bias\"][:cpu]()[:numpy]()))\n",
    "    model.decoder.w = Param(atype(weights[\"predictions.decoder.weight\"][:cpu]()[:numpy]()))\n",
    "    model.decoder.b = Param(atype(weights[\"predictions.decoder.bias\"][:cpu]()[:numpy]())) # \"predictions.decoder.bias\" is all zeros, refer to https://docs.google.com/document/d/1MMoR9aq0JYVj1hYxcWxGWzBqwq_KFtUs4zMW1-fLsPg/edit#\n",
    "    # model.decoder.b = Param(atype(weights[\"predictions.bias\"][:cpu]()[:numpy]())) \n",
    "    model.lnorm.a = Param(atype(weights[\"predictions.LayerNorm.weight\"][:cpu]()[:numpy]()))\n",
    "    model.lnorm.b = Param(atype(weights[\"predictions.LayerNorm.bias\"][:cpu]()[:numpy]()))\n",
    "    model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5d5024c-198b-464e-bd30-9c4a64f50e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adopt performer encoder for tokens-to-token\n"
     ]
    }
   ],
   "source": [
    "rgb_backbone = T2T_ViT(tokens_type=\"transformer\", embed_dim=384, depth=14, num_heads=6, mlp_ratio=3.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c8c6e38-17d5-43df-8361-53b0b29ce3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P(Knet.KnetArrays.KnetVector{Float32}(384))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_backbone.blocks[2].norm2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "44c668d7-25f3-429c-97f7-f6f62b763983",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching getindex(::T2T_ViT, ::SubString{String})",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching getindex(::T2T_ViT, ::SubString{String})",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[116]:3",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "for (k, v) in weights\n",
    "    key = rsplit(k, \".\")\n",
    "    rgb_backbone[key[1]]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "00ed4308-3bf3-46f7-b3d8-b536703e12fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = \"blocks.13.attn.qkv.weight\"\n",
      "k = \"blocks.7.attn.qkv.weight\"\n",
      "k = \"blocks.3.mlp.fc1.bias\"\n",
      "k = \"blocks.9.attn.qkv.weight\"\n",
      "k = \"blocks.0.norm2.weight\"\n",
      "k = \"blocks.4.norm2.weight\"\n",
      "k = \"blocks.3.norm1.bias\"\n",
      "k = \"blocks.1.mlp.fc1.bias\"\n",
      "k = \"blocks.2.norm2.bias\"\n",
      "k = \"blocks.8.norm1.bias\"\n",
      "k = \"blocks.5.norm2.bias\"\n",
      "k = \"blocks.8.attn.proj.weight\"\n",
      "k = \"blocks.10.mlp.fc1.bias\"\n",
      "k = \"blocks.0.mlp.fc1.bias\"\n",
      "k = \"blocks.11.mlp.fc1.bias\"\n",
      "k = \"blocks.3.mlp.fc1.weight\"\n",
      "k = \"blocks.10.norm2.weight\"\n",
      "k = \"blocks.12.norm1.weight\"\n",
      "k = \"blocks.8.attn.qkv.weight\"\n",
      "k = \"blocks.1.norm1.weight\"\n",
      "k = \"blocks.4.mlp.fc1.bias\"\n",
      "k = \"blocks.8.mlp.fc1.weight\"\n",
      "k = \"blocks.11.mlp.fc2.bias\"\n",
      "k = \"blocks.12.mlp.fc1.bias\"\n",
      "k = \"blocks.6.attn.qkv.weight\"\n",
      "k = \"blocks.0.norm1.bias\"\n",
      "k = \"blocks.10.mlp.fc2.bias\"\n",
      "k = \"blocks.6.attn.proj.bias\"\n",
      "k = \"blocks.11.norm2.weight\"\n",
      "k = \"blocks.13.mlp.fc2.weight\"\n",
      "k = \"blocks.10.mlp.fc1.weight\"\n",
      "k = \"blocks.13.norm1.weight\"\n",
      "k = \"blocks.9.norm1.bias\"\n",
      "k = \"blocks.10.norm2.bias\"\n",
      "k = \"blocks.8.mlp.fc1.bias\"\n",
      "k = \"blocks.8.norm2.bias\"\n",
      "k = \"blocks.10.mlp.fc2.weight\"\n",
      "k = \"blocks.6.mlp.fc1.weight\"\n",
      "k = \"blocks.13.norm2.bias\"\n",
      "k = \"blocks.2.mlp.fc2.bias\"\n",
      "k = \"blocks.2.mlp.fc2.weight\"\n",
      "k = \"blocks.4.attn.proj.weight\"\n",
      "k = \"blocks.9.attn.proj.weight\"\n",
      "k = \"blocks.1.attn.qkv.weight\"\n",
      "k = \"blocks.5.norm1.bias\"\n",
      "k = \"blocks.7.mlp.fc2.weight\"\n",
      "k = \"blocks.7.attn.proj.bias\"\n",
      "k = \"blocks.9.mlp.fc2.weight\"\n",
      "k = \"blocks.4.norm2.bias\"\n",
      "k = \"blocks.10.attn.proj.weight\"\n",
      "k = \"blocks.7.attn.proj.weight\"\n",
      "k = \"blocks.10.norm1.bias\"\n",
      "k = \"blocks.0.norm1.weight\"\n",
      "k = \"blocks.2.mlp.fc1.bias\"\n",
      "k = \"blocks.7.norm2.weight\"\n",
      "k = \"blocks.10.attn.proj.bias\"\n",
      "k = \"blocks.7.norm1.weight\"\n",
      "k = \"blocks.9.attn.proj.bias\"\n",
      "k = \"blocks.5.mlp.fc2.weight\"\n",
      "k = \"blocks.9.mlp.fc1.weight\"\n",
      "k = \"blocks.5.mlp.fc2.bias\"\n",
      "k = \"blocks.3.attn.qkv.weight\"\n",
      "k = \"blocks.3.attn.proj.bias\"\n",
      "k = \"blocks.11.norm2.bias\"\n",
      "k = \"blocks.13.mlp.fc2.bias\"\n",
      "k = \"blocks.11.attn.proj.weight\"\n",
      "k = \"blocks.9.mlp.fc1.bias\"\n",
      "k = \"blocks.2.norm1.bias\"\n",
      "k = \"blocks.1.norm2.weight\"\n",
      "k = \"blocks.0.attn.proj.weight\"\n",
      "k = \"blocks.6.norm2.weight\"\n",
      "k = \"blocks.8.norm2.weight\"\n",
      "k = \"blocks.12.mlp.fc2.bias\"\n",
      "k = \"blocks.1.attn.proj.weight\"\n",
      "k = \"blocks.4.norm1.weight\"\n",
      "k = \"blocks.0.attn.proj.bias\"\n",
      "k = \"blocks.5.mlp.fc1.bias\"\n",
      "k = \"blocks.12.mlp.fc1.weight\"\n",
      "k = \"blocks.4.mlp.fc2.bias\"\n",
      "k = \"blocks.12.attn.qkv.weight\"\n",
      "k = \"blocks.7.mlp.fc2.bias\"\n",
      "k = \"blocks.5.norm2.weight\"\n",
      "k = \"blocks.13.attn.proj.bias\"\n",
      "k = \"blocks.2.norm1.weight\"\n",
      "k = \"blocks.11.attn.qkv.weight\"\n",
      "k = \"blocks.7.mlp.fc1.weight\"\n",
      "k = \"blocks.12.attn.proj.bias\"\n",
      "k = \"blocks.12.attn.proj.weight\"\n",
      "k = \"blocks.9.mlp.fc2.bias\"\n",
      "k = \"blocks.12.mlp.fc2.weight\"\n",
      "k = \"blocks.8.mlp.fc2.weight\"\n",
      "k = \"blocks.6.norm1.bias\"\n",
      "k = \"blocks.0.mlp.fc2.bias\"\n",
      "k = \"blocks.5.mlp.fc1.weight\"\n",
      "k = \"blocks.13.norm1.bias\"\n",
      "k = \"blocks.7.mlp.fc1.bias\"\n",
      "k = \"blocks.1.mlp.fc2.bias\"\n",
      "k = \"blocks.6.norm1.weight\"\n",
      "k = \"blocks.8.attn.proj.bias\"\n",
      "k = \"blocks.2.attn.proj.weight\"\n",
      "k = \"blocks.11.norm1.bias\"\n",
      "k = \"blocks.4.mlp.fc2.weight\"\n",
      "k = \"blocks.5.attn.proj.weight\"\n",
      "k = \"blocks.6.attn.proj.weight\"\n",
      "k = \"blocks.8.norm1.weight\"\n",
      "k = \"blocks.4.attn.qkv.weight\"\n",
      "k = \"blocks.3.attn.proj.weight\"\n",
      "k = \"blocks.5.attn.qkv.weight\"\n",
      "k = \"blocks.11.attn.proj.bias\"\n",
      "k = \"blocks.3.mlp.fc2.weight\"\n",
      "k = \"blocks.13.mlp.fc1.bias\"\n",
      "k = \"blocks.1.norm1.bias\"\n",
      "k = \"blocks.8.mlp.fc2.bias\"\n",
      "k = \"blocks.6.mlp.fc1.bias\"\n",
      "k = \"blocks.4.attn.proj.bias\"\n",
      "k = \"blocks.13.mlp.fc1.weight\"\n",
      "k = \"blocks.6.mlp.fc2.bias\"\n",
      "k = \"blocks.3.norm2.bias\"\n",
      "k = \"blocks.0.norm2.bias\"\n",
      "k = \"blocks.13.norm2.weight\"\n",
      "k = \"blocks.9.norm2.bias\"\n",
      "k = \"blocks.11.norm1.weight\"\n",
      "k = \"blocks.1.attn.proj.bias\"\n",
      "k = \"blocks.12.norm2.bias\"\n",
      "k = \"blocks.6.mlp.fc2.weight\"\n",
      "k = \"blocks.3.norm1.weight\"\n",
      "k = \"blocks.3.mlp.fc2.bias\"\n",
      "k = \"blocks.12.norm1.bias\"\n",
      "k = \"blocks.2.norm2.weight\"\n",
      "k = \"blocks.10.attn.qkv.weight\"\n",
      "k = \"blocks.13.attn.proj.weight\"\n",
      "k = \"blocks.4.mlp.fc1.weight\"\n",
      "k = \"blocks.0.mlp.fc2.weight\"\n",
      "k = \"blocks.2.attn.proj.bias\"\n",
      "k = \"blocks.5.norm1.weight\"\n",
      "k = \"blocks.7.norm2.bias\"\n",
      "k = \"blocks.2.mlp.fc1.weight\"\n",
      "k = \"blocks.1.mlp.fc2.weight\"\n",
      "k = \"blocks.1.norm2.bias\"\n",
      "k = \"blocks.11.mlp.fc2.weight\"\n",
      "k = \"blocks.10.norm1.weight\"\n",
      "k = \"blocks.6.norm2.bias\"\n",
      "k = \"blocks.11.mlp.fc1.weight\"\n",
      "k = \"blocks.4.norm1.bias\"\n",
      "k = \"blocks.3.norm2.weight\"\n",
      "k = \"blocks.12.norm2.weight\"\n",
      "k = \"blocks.1.mlp.fc1.weight\"\n",
      "k = \"blocks.0.mlp.fc1.weight\"\n",
      "k = \"blocks.2.attn.qkv.weight\"\n",
      "k = \"blocks.5.attn.proj.bias\"\n",
      "k = \"blocks.7.norm1.bias\"\n",
      "k = \"blocks.9.norm2.weight\"\n",
      "k = \"blocks.9.norm1.weight\"\n",
      "k = \"blocks.0.attn.qkv.weight\"\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/mertcokelek/Downloads/80.7_T2T_ViT_t_14.pth.tar\"\n",
    "weights = torch.load(model_path, map_location=torch.device(\"cpu\"))[\"state_dict_ema\"];\n",
    "# weight_sample = Param(atype(weights[\"state_dict_ema\"][\"blocks.3.mlp.fc1.weight\"][:cpu]()[:numpy]()))\n",
    "# weights = sort(collect(weights[\"state_dict_ema\"]), by = x->x[1])\n",
    "\n",
    "for (k, v) in weights\n",
    "    weight_key = rsplit(k, \".\")\n",
    "    # @show weight_key\n",
    "    if occursin(\"blocks\", k) # Block\n",
    "        block_id = parse(Int32, weight_key[2])\n",
    "        block_layer = weight_key[3] # attn, mlp, norm1, norm2.\n",
    "        block_layer__attr = weight_key[4]  # attn => qkv, proj  |   mlp => fc1, fc2    |  norm1-norm2 => weight, bias\n",
    "        @show k\n",
    "    end\n",
    "    \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
