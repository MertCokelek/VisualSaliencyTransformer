{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6ab9f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Load Pretrained Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9dee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using PyCall\n",
    "using Knet\n",
    "using Knet: atype\n",
    "@pyimport torch\n",
    "@pyimport torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214d56a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_path = \"/home/mertcokelek/Downloads/80.7_T2T_ViT_t_14.pth.tar\"\n",
    "# weights = torch.load(model_path)\n",
    "# weight_sample = Param(atype(weights[\"state_dict_ema\"][\"blocks.3.mlp.fc1.weight\"][:cpu]()[:numpy]()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878eb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30a137-39de-49c0-aeb2-e91c113daaef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **gelu.jl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8bc275d-0cf7-47b5-8243-2b14da7ac7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using AutoGrad, Knet\n",
    "using AutoGrad: @gcheck\n",
    "\n",
    "const GConstant01 = sqrt(2/pi)\n",
    "const GConstant02 = 0.044715 * sqrt(2/pi)\n",
    "const GConstant03 = GConstant01 / 2\n",
    "\n",
    "# Main definition, broadcasted version works on Arrays\n",
    "\n",
    "gelu(x::T) where T = (x/2)*(1 + tanh(T(GConstant02)*x^3 + T(GConstant01)*x))\n",
    "gelu(x::Int) = gelu(convert(Float32, x))\n",
    "geluback(x::T,dy::T) where T = dy*(T(0.5)*tanh(T(GConstant02)*x^3 + T(GConstant01)*x) + (T(0.0535161)*x^3 + T(GConstant03)*x)*(1/cosh(T(GConstant02)*x^3 + T(GConstant01)*x))^2 + T(0.5))\n",
    "\n",
    "\n",
    "# This defines gelu for AutoGrad\n",
    "\n",
    "@primitive  gelu(x),dy  geluback.(x,dy)\n",
    "\n",
    "import Base.Broadcast: broadcasted\n",
    "import Knet: KnetArray\n",
    "\n",
    "function KnetArray(x::CuArray{T,N}) where {T,N}\n",
    "    p = Base.bitcast(Knet.Cptr, x.ptr)\n",
    "    k = Knet.KnetPtr(p, sizeof(x), gpu(), x) \n",
    "    KnetArray{T,N}(k, size(x))\n",
    "end\n",
    "\n",
    "broadcasted(::typeof(gelu),x::KnetArray) = KnetArray(gelu.(CuArray(x)))\n",
    "broadcasted(::typeof(geluback),x::KnetArray,dy::KnetArray) = KnetArray(geluback.(CuArray(x),CuArray(dy)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcde7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **transformer_block.jl**\n",
    "**get_sinusoid_encoding, Mlp, Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c27e7-7874-4154-8553-8197491427a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get_sinusoid_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00f124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: atype\n",
    "struct get_sinusoid_encoding \n",
    "    w \n",
    "end\n",
    "\n",
    "function get_sinusoid_encoding(n_position, d_hid; λ=10000, atype=atype())\n",
    "    x = exp.((0:2:d_hid-1) .* -(log(λ)/d_hid)) * (0:n_position-1)'\n",
    "    pe = zeros(d_hid, n_position)\n",
    "    pe[1:2:end,:] = sin.(x)\n",
    "    pe[2:2:end,:] = cos.(x)\n",
    "    get_sinusoid_encoding(atype(pe))\n",
    "end\n",
    "\n",
    "function (l::get_sinusoid_encoding)(x)\n",
    "    x .+ l.w[:,1:size(x,2)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e24543-10be-44b0-96e5-637534e7a737",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f71dda1-eaee-4eb5-86e5-42665ac94aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet.Layers21: dropout\n",
    "mutable struct Mlp\n",
    "    in_features; hidden_features; out_features; act_layer; drop;\n",
    "    \n",
    "    fc1; act; fc2;\n",
    "    function Mlp(in_features; hidden_features=nothing, out_features=nothing, act_layer::Function=gelu, drop=0.)\n",
    "        self = new(in_features, hidden_features, out_features, act_layer, drop)\n",
    "        \n",
    "        out_features = out_features == nothing ? in_features : out_features\n",
    "        hidden_features = hidden_features == nothing ? in_features : hidden_features\n",
    "        \n",
    "        self.fc1 = Linear(in_features, hidden_features)\n",
    "        self.act = act_layer#() ????\n",
    "        self.fc2 = Linear(hidden_features, out_features)\n",
    "        self.drop = drop # called in forward pass\n",
    "        return self\n",
    "    end\n",
    "    \n",
    "    function (self::Mlp)(x)\n",
    "        x = dropout(self.act(self.fc1(x)), drop)\n",
    "        x = dropout(self.fc2(x), drop)\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d93d71-68fb-4c4c-93c2-e52ce46dba9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1d4a19-7021-4a43-811a-ae79fe1257ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Attention\n",
    "    dim; num_heads; qkv_bias; qk_scale; attn_drop; proj_drop;\n",
    "    \n",
    "    scale; qkv; proj;\n",
    "    function Attention(;dim=784, num_heads=8, qkv_bias=false, qk_scale=false, attn_drop=0., proj_drop=0.)\n",
    "        self = new(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim ÷ num_heads\n",
    "        \n",
    "        self.scale = qk_scale || head_dim^-0.5\n",
    "        \n",
    "        self.qkv = Linear(dim, dim*3, bias=qkv_bias)\n",
    "        # self.attn_drop = dropout(attn_drop) # dropouts applied at forward pass\n",
    "        self.proj = Linear(dim, dim)\n",
    "        # self.proj_drop = dropout(proj_drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function transposedims(x, dim1, dim2)\n",
    "    \"\"\"\n",
    "    Helper function equivalent to torch.Tensor.transpose(dim1, dim2)\n",
    "    \n",
    "    \"\"\"\n",
    "    size_ = [i for i in size(x)]\n",
    "    dims = length(size_)\n",
    "    dim1_ = dim1 > 0 ? dim1 : dims+dim1+1\n",
    "    dim2_ = dim2 > 0 ? dim2 : dims+dim2+1\n",
    "    \n",
    "    size_[dim1_] = dim2_\n",
    "    size_[dim2_] = dim1_\n",
    "    \n",
    "    return permutedims(x, size_)\n",
    "end\n",
    "\n",
    "function(self::Attention)(x)\n",
    "    B, N, C = size(x)\n",
    "    qkv = permutedims(reshape(self.qkv(x), (B, N, 3, self.num_heads, C ÷ self.num_heads)), (3,1,4,2,5))\n",
    "    q, k, v = qkv[1], qkv[1], qkv[2]\n",
    "    \n",
    "    attn = (q * transposedims(k, -2, -1)) .* self.scale\n",
    "    softmax_dim = length(size(attn))\n",
    "    attn = softmax(attn, dims=softmax_dim)\n",
    "    attn = dropout(attn, self.attn_drop)\n",
    "    \n",
    "    x = reshape(transposedims(attn * v, 1, 2), (B, N, C))\n",
    "    x = self.proj(x)\n",
    "    x = dropout(x, self.proj_drop)\n",
    "    return x\n",
    "end \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9f02e-b4e8-4c86-b0bc-6e52772527a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ab6eb2-6b74-43e1-8885-3e1332e1582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Block\n",
    "    dim; num_heads; mlp_ratio; qkv_bias; qk_scale; drop; attn_drop;\n",
    "    drop_path; act_layer; norm_layer;\n",
    "    \n",
    "    norm1;\n",
    "    attn;\n",
    "    norm2;\n",
    "    mlp;\n",
    "    \n",
    "    function Block(;dim=784, num_heads=8, mlp_ratio=4.0, qkv_bias=false, qk_scale=nothing, drop=0., attn_drop=0.,\n",
    "        drop_path=0., act_layer::Function=gelu, norm_layer=\"LayerNorm\")\n",
    "    \n",
    "        self = new(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer, norm_layer)\n",
    "        \n",
    "        @assert norm_layer == \"LayerNorm\"\n",
    "        self.norm1 = LayerNorm(dim)\n",
    "        \n",
    "        # TODO: Attention\n",
    "        self.attn =  Attention(dim=dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        \n",
    "        # TODO: DropPath\n",
    "        # self.drop_path = drop_path > 0.? DropPath(drop_path) : nn.Identity()  # ????\n",
    "                \n",
    "        self.norm2 = LayerNorm(dim)\n",
    "        mlp_hidden_dim = convert(Int, dim * mlp_ratio)\n",
    "        self.mlp = Mlp(dim; hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        return self\n",
    "    end\n",
    "end\n",
    "\n",
    "function (self::Block)(x)\n",
    "    # x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "    return x\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ada5a-88b5-4eda-80ae-8ae2570b6edf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **utils.jl**\n",
    "**Linear, \n",
    "LayerNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91802a8c-3439-401d-8ed7-0a2eb7cc7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LayerNorm; a; b; ϵ; end\n",
    "\n",
    "function LayerNorm(dmodel; eps=1e-6)\n",
    "    a = param(dmodel; init=ones)\n",
    "    b = param(dmodel; init=zeros)\n",
    "    LayerNorm(a, b, eps)\n",
    "end\n",
    "\n",
    "function (l::LayerNorm)(x, o...)\n",
    "    μ = mean(x,dims=1)\n",
    "    σ = std(x,mean=μ,dims=1)\n",
    "    ϵ = eltype(x)(l.ϵ)\n",
    "    l.a .* (x .- μ) ./ (σ .+ ϵ) .+ l.b # TODO: doing x .- μ twice?\n",
    "end\n",
    "\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(input::Int,outputs...; bias=true)\n",
    "    Linear(param(outputs...,input), bias ? param0(outputs...) : nothing)\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    W1,W2,X1,X2 = size(l.w)[1:end-1], size(l.w)[end], size(x,1), size(x)[2:end]; \n",
    "    @assert W2===X1\n",
    "    y = reshape(l.w,:,W2) * reshape(x,X1,:)\n",
    "    y = reshape(y, W1..., X2...)\n",
    "    if l.b !== nothing; y = y .+ l.b; end\n",
    "    return y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ace3c2-64e9-4e79-9e49-cbaf4c779eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **t2t_vit.jl**\n",
    "**T2T_module, T2T_ViT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021b011-15be-4017-96c5-b0fdc7927f82",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T2T_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0772d498-00d7-4907-aa61-ede6de803151",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct T2T_module\n",
    "    \"\"\"\n",
    "    Tokens-to-Token encoding module\n",
    "    \"\"\"\n",
    "    img_size; tokens_type;\n",
    "    in_chans\n",
    "    embed_dim; token_dim;\n",
    "    \n",
    "    \n",
    "    soft_split0; soft_split1; soft_split2\n",
    "    attention1; attention2\n",
    "    project\n",
    "    num_patches\n",
    "    \n",
    "    function T2T_module(img_size=224, \n",
    "                        tokens_type=\"performer\", \n",
    "                        in_chans=3, \n",
    "                        embed_dim=768, \n",
    "                        token_dim=64)\n",
    "        self = new(img_size, tokens_type, in_chans, embed_dim, token_dim)\n",
    "        \n",
    "        if tokens_type == \"performer\"\n",
    "            println(\"adopt performer encoder for tokens-to-token\")\n",
    "            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
    "            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "            \n",
    "            # Token_performer TODO\n",
    "#             self.attention1 = Token_performer(dim=in_chans*7*7, in_dim=token_dim, kernel_ratio=0.5)\n",
    "#             self.attention2 = Token_performer(dim=token_dim*3*3, in_dim=token_dim, kernel_ratio=0.5)\n",
    "            self.project = Linear(token_dim * 3 * 3, embed_dim)\n",
    "        end\n",
    "        \n",
    "        self.num_patches = (img_size ÷ (4 * 2 * 2)) * (img_size ÷ (4 * 2 * 2))  # there are 3 soft split, stride are 4,2,2 seperately\n",
    "\n",
    "        return self\n",
    "        \"\"\"\n",
    "        TODO / NOTE: initially, let's just consider 'performer' model. 'transformer' & 'convolution' can be done later.\n",
    "        TODO: Unfold currently taken from PyTorch. Might need conversion between datatypes.\n",
    "        \"\"\"\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function (self::T2T_module)(x)\n",
    "    # step0: soft split\n",
    "    x = permutedims(self.soft_split0(x), [2, 3])\n",
    "    \n",
    "    # x [B, 56*56, 147=7*7*3]\n",
    "    # iteration1: restructurization/reconstruction\n",
    "    \n",
    "    x_1_4 = self.attention1(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_4, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split1(x), [2, 3])\n",
    "\n",
    "    # iteration2: restructurization/reconstruction\n",
    "    x_1_8 = self.attention2(x)\n",
    "    B, new_HW, C = size(x_1_4)\n",
    "    x = reshape(permutedims(x_1_8, [2, 3]), (B, C, convert(Int, sqrt(new_HW)), convert(Int, sqrt(new_HW))))\n",
    "    # iteration1: soft_split\n",
    "    x = permutedims(self.soft_split2(x), [2, 3])\n",
    "\n",
    "    # final_tokens\n",
    "    x = self.project(x)\n",
    "    \n",
    "    return x, x_1_8, x_1_4\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60bc55-1c42-48b1-b695-1608d583b7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T2T_ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a43185d-8d84-4f21-b8df-963f04a7b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adopt performer encoder for tokens-to-token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(1000,784)), P(Knet.KnetArrays.KnetVector{Float32}(1000)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct T2T_ViT; \n",
    "    img_size; tokens_type\n",
    "    in_chans; num_classes; num_features\n",
    "    embed_dim; depth\n",
    "    num_heads; mlp_ratio\n",
    "    qkv_bias; qk_scale\n",
    "    drop_rate; attn_drop_rate; drop_path_rate; \n",
    "    norm_layer;\n",
    "\n",
    "    blocks\n",
    "    norm\n",
    "    head\n",
    "    tokens_to_token\n",
    "    cls_token\n",
    "    pos_embed\n",
    "\n",
    "    function T2T_ViT(img_size=224, \n",
    "            tokens_type=\"performer\", \n",
    "            in_chans=3, num_classes=1000,\n",
    "            embed_dim=784,depth=12,\n",
    "            num_heads=12, mlp_ratio=4., \n",
    "            qkv_bias=false, qk_scale=false, \n",
    "            drop_rate=0., attn_drop_rate=0., drop_path_rate=0., \n",
    "            norm_layer=\"LayerNorm\"\n",
    "            )\n",
    "            \n",
    "        self = new(img_size, tokens_type, \n",
    "                    in_chans, num_classes, \n",
    "                    embed_dim,depth,num_heads,\n",
    "                    mlp_ratio,qkv_bias,qk_scale,\n",
    "                    drop_rate,attn_drop_rate,\n",
    "                    drop_path_rate,norm_layer)\n",
    "        self.num_features = embed_dim\n",
    "\n",
    "        self.tokens_to_token = T2T_module()\n",
    "        self.cls_token = convert(KnetArray, zeros(1,1,768))\n",
    "        self.pos_embed = reshape(get_sinusoid_encoding(16, 784).w, (1, 16, 784))  #\n",
    "        \n",
    "\n",
    "        # NOTE: In PyTorch implementation, dropout is added as a portable layer.\n",
    "        # We will add it as an operation, in the forward pass. \n",
    "        # UPDATE: probably we won't need it, since we'll use pretrained weights. \n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        dpr = [i for i in 0:0.25/11:0.25] # Stochastic depth decay rule\n",
    "\n",
    "        self.blocks = [Block(dim=embed_dim, \n",
    "                            num_heads=num_heads, \n",
    "                            mlp_ratio=mlp_ratio, \n",
    "                            qkv_bias=qkv_bias, \n",
    "                            qk_scale=qk_scale,\n",
    "                            drop=drop_rate, \n",
    "                            attn_drop=attn_drop_rate,\n",
    "                            drop_path=dpr[i], \n",
    "                            act_layer=gelu,\n",
    "                            norm_layer=norm_layer) for i in 1:depth]\n",
    "        @assert norm_layer == \"LayerNorm\"                   \n",
    "        self.norm = LayerNorm(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = Linear(embed_dim, num_classes)\n",
    "        \n",
    "#         For initializing weights, probably we won't need the rest.\n",
    "#         We will use pretrained weights.\n",
    "    \n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function expand(x, B)\n",
    "    \"\"\"\n",
    "    Helper function for torch.Tensor.expand()\n",
    "    expand(x, B) is equivalent to x.expand(B, -1, -1)\n",
    "    \"\"\"\n",
    "    res = x\n",
    "    dimx = length(size(x))\n",
    "    for i in 1:B-1\n",
    "        res = cat(res, x, dims=dimx+1)\n",
    "    end\n",
    "    permute_order = [i for i in 1:dimx]\n",
    "    # @show permute_order\n",
    "    insert!(permute_order, 1, dimx+1)\n",
    "    # @show permute_order, dimx, size(x)\n",
    "    # @show size(res)\n",
    "    res = permutedims(res, permute_order)\n",
    "end\n",
    "\n",
    "function (self::T2T_ViT)(x)\n",
    "    B = size(x)[1]\n",
    "    x, x_1_8, x_1_4 = self.tokens_to_token(x)\n",
    "    \n",
    "    cls_tokens = expand(self.cls_token, B) # self.cls_token.expand(B, -1, -1) in Torch\n",
    "    x = hcat(cls_tokens, x)\n",
    "    x += self.pos_embed\n",
    "    # x = self.pos_drop # Dropout will not be used for now.\n",
    "    \n",
    "    # T2T-ViT backbone\n",
    "    for blk in self.blocks\n",
    "        x = blk(x)\n",
    "    end\n",
    "    \n",
    "    x = self.norm(x)\n",
    "    return x[:,2:end,:], x_1_8, x_1_4\n",
    "end    \n",
    "\n",
    "\n",
    "dummyT = T2T_ViT()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
